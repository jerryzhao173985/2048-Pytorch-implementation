```
## 4 cov layer , initial SGD learning rate and max 1024 linear channel;
Using (1,2) and (2,1) filter is definitely effective!!
(Because in the 16 in_channel there are rotations of this game)
 
0 (162, 128, 1608)

1 (227, 256, 2668)

2 (184, 128, 1796)

3 (123, 128, 1144)

4 (443, 512, 6228)

5 (172, 128, 1740)

6 (168, 128, 1604)

7 (120, 64, 1004)

8 (210, 256, 2440)

9 (238, 256, 2788)

10 (128, 64, 1084)

11 (154, 64, 1356)

12 (180, 128, 1764)

13 (106, 64, 884)

14 (144, 128, 1356)

15 (290, 256, 3564)

16 (403, 512, 5744)

17 (95, 64, 724)

Max: 6228

```







```
0 (150, 128, 1428)

1 (170, 128, 1612)

2 (212, 256, 2536)

3 (245, 256, 3036)

4 (112, 64, 964)

5 (193, 128, 1896)

6 (166, 128, 1776)

7 (86, 32, 596)

8 (140, 128, 1292)

9 (164, 128, 1564)

10 (209, 256, 2464)

11 (326, 256, 3944)

12 (274, 256, 3324)

13 (238, 256, 2892)

14 (156, 64, 1376)

15 (179, 128, 1752)

16 (170, 128, 1672)

17 (56, 32, 312)

19 (287, 256, 3364)

20 (221, 256, 2540)

Max: 3944
```



Using Adam with beta1 = 0.5: (same learning rate)

The worst!!!

```
1 (108, 64, 908)

2 (93, 64, 668)

3 (49, 16, 220)

4 (136, 64, 1140)

5 (57, 32, 300)

6 (44, 16, 192)

7 (85, 64, 620)

8 (62, 32, 352)

9 (133, 64, 1128)

10 (93, 64, 720)

```



Adam with default betas:

```
0 (138, 128, 1312)

1 (184, 128, 1824)

2 (182, 128, 1784)

3 (183, 128, 1792)

4 (254, 256, 3096)

5 (118, 64, 976)

6 (113, 128, 1048)

7 (207, 256, 2436)

8 (227, 256, 2648)

9 (142, 64, 1264)

10 (267, 256, 3088)

11 (112, 128, 1076)

12 (189, 256, 2192)

13 (228, 256, 2600)

14 (221, 256, 2616)

15 (134, 128, 1316)

16 (66, 64, 456)

17 (206, 256, 2420)

18 (410, 128, 2452)

19 (145, 128, 1424)

20 (132, 128, 1240)

21 (123, 64, 1024)

23 (219, 128, 2236)

```



Notice an import ant thing in calculating loss!! (criterion)

The oprediction and target should be of same shape ; if one is size([8]) and another is size([8,1]), then there is an issue!!  Because of broadcasting, the size([8]) - size([8,1]) = size([8,8]) !!!